---
title: 'Model 3: Boosted tree'
author: Luca Baggi
date: '2021-01-14'
slug: []
categories: []
tags: []
draft: true
ShowToc: true
---

Our decision tree had a worse performance than our baseline multinomial classifier: to improve its performance, we shall put it through an ensamble method, aiming at reducing its bias.

# Brief Intro to Ensamble Methods

Trees generally have low bias, but high variance. They generally behave the opposite of (linear) regressions, which have low variance and high bias. We can tip the bias-variance trade-off in our favour using ensamble methods.

Ensambling means 'grouping together' more of the same learner and average their performances. Trees are easy to ensamble and the simplest example of this are bagged trees (boostrap aggregated trees).

Bagged trees average out the performance of non-pruned trees trained on bootstrap samples of the training dataset. Random forest are an extension of this procedure, aimed at dealing with correlation between trees.

Bagged trees are correlated because they employ the same number of variables to create a split on each node. Random forests are more 'wild' and heterogeneous: as a forest, they display a broader variety of trees. This practically means that at each node, `m` random predictors are selected, from which the best candidate is used for the split. Actually, bagging is a subset of random forests, where `m = p`. This may seem weird, but it actually reduces the variance, at the cost of interpretability.

# Load Packages

```{r, message=FALSE}
library(tidymodels)
library(tidyverse)

# for downsampling
library(themis)

# for visualising importance of predictors
library(vip)
```

# Load Data

```{r}
trips <-
  read_csv(
    'https://raw.githubusercontent.com/baggiponte/escooters-louisville-r/main/data/escooters_od_reduced.csv',
    col_types = cols(
      StartTime = col_datetime(format = '%Y-%m-%dT%H:%M:%SZ'),
      Covid = col_factor(),
      StartNH = col_factor(),
      EndNH = col_factor()
    ))
```

# Split in Train and Test Data

Class imbalance needs to be addressed with stratified sampling, as we did in the earlier post:

```{r}
set.seed(42)

trips_split <- initial_split(trips, strata = EndNH)

trips_train <- training(trips_split)
trips_test <- testing(trips_split)
```

# Define the Recipe

This time, we shall set `under_ratio` to 1.

```{r}
trips_recipe <- trips_train %>%
  recipe(EndNH ~ .) %>%
  # problem: step_date does not extract times!
  step_mutate(HourNum = format(strptime(StartTime,'%Y-%m-%d %H:%M:%S'),'%H')) %>%
  # turn it into a factor
  step_string2factor(HourNum) %>%
  # create factors out of StartTime
  step_date(StartTime, features = c('dow', 'month', 'year')) %>%
  # create holiday dummy:
  step_holiday(StartTime, holidays = timeDate::listHolidays("US")) %>%
  # remove StartTime col
  step_rm(StartTime) %>%
  # turn factor-features into binary dummies (i.e. one per column: 1-0):
  step_dummy(all_nominal(), -all_outcomes()) %>%
  # remove predictors with zero variance:
  step_zv(all_predictors()) %>%
  # downsample the data: each class is as numerous as the least represented
  themis::step_downsample(EndNH, under_ratio = 1)
```

# Define a Model with the Parameters to Tune

There are quite a few parameters to set here! But we don't have to tune all of them.

For example, boosted trees use shallow trees. The defaults are the following, and are inherited from the default package, `xgboost`:

* `tree_depth` is 6.
* `learn_rate` is 0.3.
* `mtry` is the number of random predictors among which the best is chosen at each node. `xgboost` chooses among the 100% of parameters by default.
* `min_n` is the minimum number of observations in the leaves/nodes after which a split occurs.
* `loss_reduction` is set to 0 and is the minimum loss reduction required to make a further partition on a leaf node of the tree. The larger parameter is, the more conservative the algorithm will be.
* `sample_size` is the proportion of the training test that goes into each tree, default 100%.

`xgboost` has also regularisation parameters: `reg_lambda = 1` is the default L2 regularisation (Ridge), `alpha = 0` is the default LASSO regularisation. We might want to go full `ridge`, as what we want to achieve is the opposite of the following effect:

> as Î» increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.

In other words, we are willing to have a bit more variance to reduce the error. Ideally, this trade-off would not bite us back in the end especially if we were using the full dataset.

Another parameter that is usually tuned is the number of trees (or learners, when not boosting trees), which should not be too high to avoid overfitting (although overfitting is more sensitive to other parameters being out of tune).

```{r}
btree_to_tune <- 
  boost_tree(
    trees = 1000,
    learn_rate = tune(),
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

btree_to_tune
```

# Add the Recipe and the Model to the Tuning Workflow

Once again, let's add the two to make the workflow:

```{r}
btree_tuning_workflow <-
  workflow() %>%
  add_model(btree_to_tune) %>%
  add_recipe(trips_recipe)

btree_tuning_workflow
```

# Hyper-parameters Tuning

## Define the tuning grid

Define a grid with our parameters of interest: `cost_complexity` is analogue to the measure of regularisation in linear models. `tree_depth` indicates the number of nodes.

```{r}
btree_tune_grid <- tibble(learn_rate = 3 * 10^seq(-4, -2, length.out = 3))

btree_tune_grid
```

We cannot use too many combinations, due to hardware constraints.

## `k-fold` Cross Validation

Defaults are `v = 10` and `repeats = 1`: we won't stick to these totally, to be consistent with our previous model. Folds are stratified, to ensure equal representation.

```{r}
set.seed(42)

btree_cv_folds <-
  trips_train %>%
  vfold_cv(v = 5, strata = "EndNH")

btree_cv_folds
```

# Cross validation for parameter tuning

Let's initialise the resample:

```{r, results='hide', message=FALSE, warning=FALSE}
set.seed(42)

btree_resampling <-
  btree_tuning_workflow %>%
  tune_grid(
    resamples = btree_cv_folds,
    grid = btree_tune_grid
  )
```

And sort the models by their metrics:

```{r}
tree_resampling %>%
  collect_metrics() %>%
  arrange(desc(mean))
```

Performances are quite similar to those of the logistic, at least on the training set. Cost complexity does not affect performance, apparently. Let's plot it!

```{r}
tree_resampling %>%
  autoplot()
```

We see quite a weird pattern. Accuracy increases with cost complexity when the number of branches/leaves/nodes is high. But there's a reason for this: with a low cost complexity, a high number of nodes means overfitting! In other words, a model with high cost complexity and a greater number of trees appears equivalent to a smaller tree.

But let's look at the receiving operator area under the curve: clearly, a higher cost complexity weakens the model!

This is a great example of why one should use more than one metric.

```{r}
best_tree <-
  tree_resampling %>%
  select_best('roc_auc')

best_tree
```

# Finalise the Workflow and Last Fit

```{r}
final_tree_workflow <-
  tree_tuning_workflow %>%
  finalize_workflow(best_tree)

final_tree_workflow
```

And the last fit, which computes simultaneously fitting on the training set and predicting the classes in the test set:

```{r}
final_tree_fit <-
  final_tree_workflow %>%
  last_fit(trips_split)
```

Let's get the final metrics:

```{r}
final_tree_fit %>%
  collect_metrics()
```
Performance under the accuracy is low, but the AUC indicates that the model performs much better than our logistic regression!

# Model Evaluation and Considerations

Let's look at the confusion matrix:

```{r}
final_tree_fit %>%
  collect_predictions() %>%
  conf_mat(EndNH, .pred_class) %>%
  autoplot(type = 'heatmap')
```

Upon confronting the two confusion matrices, though, it appears that the tree performs worse! This is to be expected, as the trees have on average high variance and lower biases and so the model we chose might have overfit its resample. We shall improve on this model with the next!

And `pluck` the `.workflow` column to get the importance of each variable:

```{r}
final_tree_fit %>%
  pluck('.workflow', 1) %>%
  pull_workflow_fit() %>%
  vip()
```

Note how this model uses different predictors than the regression: more of the neighbourhoods and less of the holidays.